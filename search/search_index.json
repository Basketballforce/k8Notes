{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"k8/","text":"Beginner K8 Course Based on Udemy course linked here Course Overview 2 additional courses available. One for admins. Prep for admin cert. One for devs. Prep for dev cert kubernetes Overview Built by google for Container Orchestration Container: virtualized env (docker, podman) lxd, lxe, etc. are types of containers Run linux os kernel underneath (shared linux kernel) Image: Template for creating containers Container: Instance of a container Orchestration (kubernetes, docker swarm, Apache Mesos): Platform that mananges load balancing, networking, deploying Docker swarm lacks advanced features but is easy to setup Kubernetes (most popular) difficult to manage and get started but has advanced features Mesos=kubernetes but less popular Kubernetes Architechture https://kubernetes.io/docs/concepts/overview/components/ Nodes = Physical or virt machine k8 is installed on and sits on top of worker machine where containers are launched (minions) Cluster = set of nodes grouped together for fallback/redundency and load-balancing Master = another node with k8 installed. Watches over and manages cluster K8 installation includes: api server = frontend which interacts with users and cli interface etcd Service = distributed key value store with data used to manage cluster. Prevents master conflicts kubelet service = agent running on nodes in cluster. Makes sure containers are running as expected Container Runtime = software used to run containers. I.E Docker Controller = brain behind orchestration. Fallback listener and redudency manager. Create new containers Scheduler = distribute work across nodes Master vs worker node docker/rkt/cti-o containers run on worker Master has a kube-apiserver to manage worker nodes Nodes have kubelet agent used to communicate to master api Master has etcd, controller and scheduler CLI tool Kubectl used to manage k8 cluster and nodes, etc. K8 Setup and Install Options Local - Minikube - MicroK8s - kubeadm (kube admin) - for production grade k8 clusters - kind Cloud - AWS, GCP, Azure, etc. Minikube - bundles k8 components in one package for easy setup - CLI helps with management kubernetes concepts - PODs ### PODs - container object that holds a single containerized application within a worker node - Can hold multiple containerized applications of different types. Just no duplicates of the same application - For scaling more pods are created rather than adding more containers to a pod - Having multiple containers in a pod is a rarer implementation type - smallest creatable obj in k8 - Worker nodes can have multiple pods. Clusters can have many nods - Containers in the same pod have access to the same localhost/network #### POD Demo - kubectl run nginx --image=nginx - creates & runs nginx pod. By default pulls from docker hub - kubectl get pods - check status of pods - kubectl describe pod nginx -o wide - get description of pod YAML - key value pair - spaces between colon - yaml : annoying - array and list - arrayName: - item1 - item2 - arrayName ends with a colon and newline - items are prepended with a dash - dictionary/map - mapName: item1: value1 item2: value2 - mapName ends with colon and newline - items are indented - blank space must be consistent - if extra spaces are added, then the item becomes a child of the previous ex: Banana: calories: 88 fat: 33 - fat is a child of calories and calories is a child of banana (which would cause an error since calories is mapped to a value) - can mix and match/nest list and dictionaries - Dictionaries are unordered while list are ordered - # are used to make commented lines Create Pod w/yaml - K8 uses yaml files for pods, services, replicas, deployments, etc. - Always Includes/requires: 1. apiVersion: version of k8 api we want to use - ex: v1, apps/v1 2. kind: type of object we wish to create - Pod, ReplicaSet, Deployment, Service 3. metadata: data about the object. - name, labels, etc. ex: meatdata: name: myapp-dev labels: app: myapp type: front-end 4. spec: extra info about the specific object we are creating. Different for different objects (pods, ReplicaSets, etc.) ex for pods: spec: containers - name: nginx-container image: nginx - name: second-container image: api-one ## Create Pods w/Yaml Demo - Create pod yaml apiVersion: v1 kind: Pod metdata: name: nginx labels: app: nginx tier: frontend spec: containers: - name: nginx image: nginx - apply pod yaml - kubectl create/apply -f pod.yaml ## Debugging yaml - IDE tooling helps with writing - atom, pycharm, netbeans, vscode - To debug use IDE extensions - Vscode plugin/extension \"Yaml\" by redhat - in extension settings add a yaml schema of: ```\"kubernetes\" : \"*.yaml\"``` - to set yaml schema to kubernetes config - vscode plugin also shows yaml layout in outline section of vscode Example Yaml apiVersion: v1 kind: Pod metadata: name: postgres labels: tier: db-tier spec: containers: - name: postgres image: postgres Code Labs ## Create a yaml based off a dry run of an image kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml ## Edit created/running pods yaml with kubectl edit pod pod_name Replication Controller and ReplicaSets Controllers Monitor k8 objects and respond accordingly ### Replication controller - Run multiple instances of a pod to provide high availabilty in case of pod failures - Can be used as well to create additional pods to load balance and scale - Pod to be replicated are specified by the spec field in a yaml file. Everything in the pod defintion minus the version and api version. Under the spec and template like this: spec: template: podYAMLhere - After the pod yaml in the spec, you specify how many pods you want to replicate under the spec but outside the template with: replicas: 3 ### Replication Controller vs ReplicaSet - Both have the same purpose but are not the same. - Replica Controller are older method and being replaced with ReplicaSet - ReplicaSets are reccomended ### Replication Controller Example create yaml file such as rc-defintion.yaml files for it apiVersion: v1 kind: ReplicaController metadata: name: myapp labels: app: myapp type: front-end spec: template: podYAMLHERE replicas: 3 ### Create controller - ```kubectl create -f rc-definition.yaml``` ### Get controller - ```kubectl get replicationcontroller``` ### ReplicaSet Similar to replicacontroller except it changes: apiVersion: apps/v1 kind: ReplicaSet and adds a selector under the spec: selector: matchLabels: type: front-end this requires a label field under metadata that has a subfield of type: front-end that matches the selector label -> matchLabels -> type ### Get ReplicaSet ```kubectl get replicaset``` ### Labels and selectors - used to specify pods to be monitored ### Scaling Pods with ReplicaSet - 1. Change # of replicas in yaml and update - update replicas to 6 ```replicas: 6``` - then ```kubectl replace -f replicaset -f replica-set.yaml``` - 2. Specify and update with cli - ```kubectl scale -replicas=6 -f replica-set.yaml``` or - ```kubectl scale --replicas=6 replicaset myapp-replicaset``` ### Delete Replicaset and underlying pods ```kubectl delete replicaset myapp-replicaset``` ### Describe replicasSet ```kubectl describe replicaset myapp-replicaset``` Deployments - How to deploy apps in prod - Used to manage updates/rolling updates - Used for rollback policy enforcement - Used to pause upgrade and start envirionment ### Create deployment definition yaml - Base YAML is the Same as replicaset from above - except the kind is deployment ### Get deployments - ```kubectl get deployments``` - ```kubectl get all``` ### Demo - Create deployment from scratch ```kubectl create deployment my-deployment --image=myimage``` - Scale deployment created in previous command with: ```kubectl scale deployment --replicas=3 my-deployment``` ### Rollout & Versioning #### Rollouts - rollout=deployment revision - get status of rollout with ```kuebctl rollout status deployment/myapp-deployment``` - see history and revisions of rollouts run ```kubectl rollout history deployment/myapp-deployment``` - Deployment strategies - (recreate strategy) destroy all old, then add all new - DEFAULT BEHAVIOR: (rollingupdate strategy) for all pods: delete one old pod, add one new pod, then move to next pod - How to update deployment 1. use kubectl apply - make changes in deployment-defintion.yaml and apply 2. Use kubectl set image to update the image/eq command for kubectl to update - ```kubectl set image deployment/myapp-deployment \\ nginx=nginx:1.9.1``` #### Upgrades - K8 will create another replica set with updated pods and as it does this it will delete the old replicaset with the old pods - Undo upgrade - ```kubectl rollout undo deployment/myapp-deployment``` #### Demo - kuebctl create -f deployment.yaml - kuebctl rollout status deployment.apps/myapp-deployment - kubectl rollout history deployment.apps/myapp-deployment - Record cause of change shown in above history command with (need to wipe deployment and restart from with kubectl delete deployment): - kubectl create -f deployment.yaml --record - records commands used to create and manage deployments - kubectl edit deployment myapp-deployment --record - --record command required to track changes - revert changes - kubectl rollout undo deployment/myapp-deployment - Undo commands to previous revisions will set the previous revision as the most recent in kubectl history if changes are tracked with --record Networking - Nodes are given an ip address. Used to access and ssh into node - Node ip address can be 192.168.1.xxx for example. - Minikube runs its own node within the host system for example - Pods within nodes are given their own ip address as well - A 10.244.x.x ip is addressed to it and other pods in the node - As such pods can communicate to other pods with 10.244.0.0 - Although this is generally considered bad practice as pod ips can change and there are better ways to network. DON'T DO THIS - Multi-Node networking - k8 doesn't setup cluster networking for us. - k8 expects us to enable communication between pods without a NAT and nodes ability connect to all containers without NAT - Multiple pre-built tooling to do this for us - flannel, vmware nsxt, cilium, cicso aci networks, Calico - ex: For local systems: calico, flannel work - ex: for vmware: vmware nsxt Services - Enable communication between components within and outside application - Help connect applications together with other applications and users - For example: Makes frontend-available to users and to backend containers. - Allow backend service to communicate to db ### External Communication - how do we access a wepage externally from running cluster and pod - EX: k8 cluster running locally - WITH A cluster running on 192.168.1.2, external host computer running on 192.168.1.10, pod running on 10.244.0.2 - Could ssh into k8 node at 192.168.1.2 we could curl the internal pod but this doesn't give us easy/true external access - We need something in the middle to map requests to the node from our external host to the pod - This is what k8 services do! - k8 service can listen to node port and forward it to the pod - called a NodePort service ### Service types - NodePort: maps requests between node and pods - ClusterIP: uses a virtual ip in the cluster to enable communication between different services running - LoadBalancer: Provisions a load balancer in supported cloud providers - distribute load accross servers ### NodePort - Map port on node to port on the pod - Involves three ports - pod port == TargetPort - service port (service is like a virtual server inside the node that has its own ip (cluster ip of the service)) == Port - node port = NodePort - with a range of ports from 30000 - 32767 ### Creating a service #### NodePort Service EX: service-definition.yml apiVersion: v1 kind: Service metadata: name: myapp-service spec: type: NodePort ports: - targetPort: 80 port: 80 nodePort: 30008 selector: app: myapp type: front-end - the only mandatory field is port - if no port is supplied for targetPort, then it will be assigned to port - if no nodePort is assigned, then a random one will be given - specify the pod with labels and selectors - create service with ```kubectl create -f service-definition.yml``` - see service with ```kubectl get services``` - We can now access the pod using the node ip address and targetPort - ex: ```curl http://192.168.1.2:30008``` - This example demonstrates a service mapped to single pod but this isn't always the case. I.E multiple pods - Example: multiple pods up for load-balance and redundency with the same selector and label - K8 would forward all pods to the service and node using a random session affinty load balance method - What about a deployment spread out between multiple nodes - K8 will create service that spans all nodes and maps targetport to same nodeport on all clusters #### Service Demo - ```kubectl get svc``` = ```kubectl get service``` - If on minkube, we can run the minikube service command to get the url of the service - select a service (such as myapp-service) from kubectl get svc and run: - ```minkube service myapp-service --url``` #### ClusterIP Service - used to setup communication between different running services using a virtual cluster ip - defualt type of service. If not specified in yaml will default to this - EX: apiVersion: v1 kind: Service metadata: name: back-end spec: type: ClusterIP ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end #### Services - Load Balancer - Common steps for networking in a cluster 1. Create deployments 2. Create services (clusterIP) 3. Create services (LoadBalancer) - Used to create custom url and \"domain to access application\" - Essentially acting as a load-balancer or proxy for the node urls (192.168.xxx.xxx) - LoadBalancers are available natively with cloud providers/will only work with supported cloud providers - in unsupported envs it would have the same affect as nodePort EX: apiVersion: v1 kind: Service metadata: name: myapp-service spec: type: LoadBalancer ports: - targetPort: 80 port: 80 nodePort: 30008 Create a service from another service using ```kubectl expose``` - example: kubectl expose deployment my-app-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > svc.yaml ### Microservices - docker run --links is used to link docker containers together - example to link web app to redis container already running - ```docker run -d --name=vote -p 5000:80 --link redis:redis voting app``` - maps redis container to a hostname, in this case redis - --link container:hostname - using the links option is deprecated!!! - Example goal - deploy containers, enable connectivity and external access - Steps - Deploy PODs - Connect services (ClusterIp) - Configure services for external access (NodePort) - TODO - research purpose of selector/matchlabel in replicaSets - what is replicasets.apps used for in kubectl cli/command line - All of this guide assumes you are in a k8 cluster. What about switching/managing the cluster - keep in mind name or labels (such as myapp) (need to clarify whether its the name of label) are used to specify objects in kubectl cli Containers - Docker Container Orchestration Demo - Setup kubernetes kubernetes concepts - PODs | ReplicaSets | Deployment | Services Networking in kubernetes kubernetes management - Kubectl kubernetes defintion files - YAML kubernetes on cloud - AWS/GCP","title":"Home"},{"location":"k8/#beginner-k8-course","text":"Based on Udemy course linked here","title":"Beginner K8 Course"},{"location":"k8/#course-overview","text":"2 additional courses available. One for admins. Prep for admin cert. One for devs. Prep for dev cert","title":"Course Overview"},{"location":"k8/#kubernetes-overview","text":"Built by google for Container Orchestration Container: virtualized env (docker, podman) lxd, lxe, etc. are types of containers Run linux os kernel underneath (shared linux kernel) Image: Template for creating containers Container: Instance of a container Orchestration (kubernetes, docker swarm, Apache Mesos): Platform that mananges load balancing, networking, deploying Docker swarm lacks advanced features but is easy to setup Kubernetes (most popular) difficult to manage and get started but has advanced features Mesos=kubernetes but less popular","title":"kubernetes Overview"},{"location":"k8/#kubernetes-architechture","text":"https://kubernetes.io/docs/concepts/overview/components/ Nodes = Physical or virt machine k8 is installed on and sits on top of worker machine where containers are launched (minions) Cluster = set of nodes grouped together for fallback/redundency and load-balancing Master = another node with k8 installed. Watches over and manages cluster K8 installation includes: api server = frontend which interacts with users and cli interface etcd Service = distributed key value store with data used to manage cluster. Prevents master conflicts kubelet service = agent running on nodes in cluster. Makes sure containers are running as expected Container Runtime = software used to run containers. I.E Docker Controller = brain behind orchestration. Fallback listener and redudency manager. Create new containers Scheduler = distribute work across nodes Master vs worker node docker/rkt/cti-o containers run on worker Master has a kube-apiserver to manage worker nodes Nodes have kubelet agent used to communicate to master api Master has etcd, controller and scheduler CLI tool Kubectl used to manage k8 cluster and nodes, etc.","title":"Kubernetes Architechture"},{"location":"k8/#k8-setup-and-install-options","text":"","title":"K8 Setup and Install Options"},{"location":"k8/#local","text":"- Minikube - MicroK8s - kubeadm (kube admin) - for production grade k8 clusters - kind","title":"Local"},{"location":"k8/#cloud","text":"- AWS, GCP, Azure, etc.","title":"Cloud"},{"location":"k8/#minikube","text":"- bundles k8 components in one package for easy setup - CLI helps with management","title":"Minikube"},{"location":"k8/#kubernetes-concepts-pods","text":"### PODs - container object that holds a single containerized application within a worker node - Can hold multiple containerized applications of different types. Just no duplicates of the same application - For scaling more pods are created rather than adding more containers to a pod - Having multiple containers in a pod is a rarer implementation type - smallest creatable obj in k8 - Worker nodes can have multiple pods. Clusters can have many nods - Containers in the same pod have access to the same localhost/network #### POD Demo - kubectl run nginx --image=nginx - creates & runs nginx pod. By default pulls from docker hub - kubectl get pods - check status of pods - kubectl describe pod nginx -o wide - get description of pod","title":"kubernetes concepts - PODs"},{"location":"k8/#yaml","text":"- key value pair - spaces between colon - yaml : annoying - array and list - arrayName: - item1 - item2 - arrayName ends with a colon and newline - items are prepended with a dash - dictionary/map - mapName: item1: value1 item2: value2 - mapName ends with colon and newline - items are indented - blank space must be consistent - if extra spaces are added, then the item becomes a child of the previous ex: Banana: calories: 88 fat: 33 - fat is a child of calories and calories is a child of banana (which would cause an error since calories is mapped to a value) - can mix and match/nest list and dictionaries - Dictionaries are unordered while list are ordered - # are used to make commented lines","title":"YAML"},{"location":"k8/#create-pod-wyaml","text":"- K8 uses yaml files for pods, services, replicas, deployments, etc. - Always Includes/requires: 1. apiVersion: version of k8 api we want to use - ex: v1, apps/v1 2. kind: type of object we wish to create - Pod, ReplicaSet, Deployment, Service 3. metadata: data about the object. - name, labels, etc. ex: meatdata: name: myapp-dev labels: app: myapp type: front-end 4. spec: extra info about the specific object we are creating. Different for different objects (pods, ReplicaSets, etc.) ex for pods: spec: containers - name: nginx-container image: nginx - name: second-container image: api-one ## Create Pods w/Yaml Demo - Create pod yaml apiVersion: v1 kind: Pod metdata: name: nginx labels: app: nginx tier: frontend spec: containers: - name: nginx image: nginx - apply pod yaml - kubectl create/apply -f pod.yaml ## Debugging yaml - IDE tooling helps with writing - atom, pycharm, netbeans, vscode - To debug use IDE extensions - Vscode plugin/extension \"Yaml\" by redhat - in extension settings add a yaml schema of: ```\"kubernetes\" : \"*.yaml\"``` - to set yaml schema to kubernetes config - vscode plugin also shows yaml layout in outline section of vscode","title":"Create Pod w/yaml"},{"location":"k8/#example-yaml","text":"apiVersion: v1 kind: Pod metadata: name: postgres labels: tier: db-tier spec: containers: - name: postgres image: postgres","title":"Example Yaml"},{"location":"k8/#code-labs","text":"## Create a yaml based off a dry run of an image kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml ## Edit created/running pods yaml with kubectl edit pod pod_name","title":"Code Labs"},{"location":"k8/#replication-controller-and-replicasets","text":"Controllers Monitor k8 objects and respond accordingly ### Replication controller - Run multiple instances of a pod to provide high availabilty in case of pod failures - Can be used as well to create additional pods to load balance and scale - Pod to be replicated are specified by the spec field in a yaml file. Everything in the pod defintion minus the version and api version. Under the spec and template like this: spec: template: podYAMLhere - After the pod yaml in the spec, you specify how many pods you want to replicate under the spec but outside the template with: replicas: 3 ### Replication Controller vs ReplicaSet - Both have the same purpose but are not the same. - Replica Controller are older method and being replaced with ReplicaSet - ReplicaSets are reccomended ### Replication Controller Example create yaml file such as rc-defintion.yaml files for it apiVersion: v1 kind: ReplicaController metadata: name: myapp labels: app: myapp type: front-end spec: template: podYAMLHERE replicas: 3 ### Create controller - ```kubectl create -f rc-definition.yaml``` ### Get controller - ```kubectl get replicationcontroller``` ### ReplicaSet Similar to replicacontroller except it changes: apiVersion: apps/v1 kind: ReplicaSet and adds a selector under the spec: selector: matchLabels: type: front-end this requires a label field under metadata that has a subfield of type: front-end that matches the selector label -> matchLabels -> type ### Get ReplicaSet ```kubectl get replicaset``` ### Labels and selectors - used to specify pods to be monitored ### Scaling Pods with ReplicaSet - 1. Change # of replicas in yaml and update - update replicas to 6 ```replicas: 6``` - then ```kubectl replace -f replicaset -f replica-set.yaml``` - 2. Specify and update with cli - ```kubectl scale -replicas=6 -f replica-set.yaml``` or - ```kubectl scale --replicas=6 replicaset myapp-replicaset``` ### Delete Replicaset and underlying pods ```kubectl delete replicaset myapp-replicaset``` ### Describe replicasSet ```kubectl describe replicaset myapp-replicaset```","title":"Replication Controller and ReplicaSets"},{"location":"k8/#deployments","text":"- How to deploy apps in prod - Used to manage updates/rolling updates - Used for rollback policy enforcement - Used to pause upgrade and start envirionment ### Create deployment definition yaml - Base YAML is the Same as replicaset from above - except the kind is deployment ### Get deployments - ```kubectl get deployments``` - ```kubectl get all``` ### Demo - Create deployment from scratch ```kubectl create deployment my-deployment --image=myimage``` - Scale deployment created in previous command with: ```kubectl scale deployment --replicas=3 my-deployment``` ### Rollout & Versioning #### Rollouts - rollout=deployment revision - get status of rollout with ```kuebctl rollout status deployment/myapp-deployment``` - see history and revisions of rollouts run ```kubectl rollout history deployment/myapp-deployment``` - Deployment strategies - (recreate strategy) destroy all old, then add all new - DEFAULT BEHAVIOR: (rollingupdate strategy) for all pods: delete one old pod, add one new pod, then move to next pod - How to update deployment 1. use kubectl apply - make changes in deployment-defintion.yaml and apply 2. Use kubectl set image to update the image/eq command for kubectl to update - ```kubectl set image deployment/myapp-deployment \\ nginx=nginx:1.9.1``` #### Upgrades - K8 will create another replica set with updated pods and as it does this it will delete the old replicaset with the old pods - Undo upgrade - ```kubectl rollout undo deployment/myapp-deployment``` #### Demo - kuebctl create -f deployment.yaml - kuebctl rollout status deployment.apps/myapp-deployment - kubectl rollout history deployment.apps/myapp-deployment - Record cause of change shown in above history command with (need to wipe deployment and restart from with kubectl delete deployment): - kubectl create -f deployment.yaml --record - records commands used to create and manage deployments - kubectl edit deployment myapp-deployment --record - --record command required to track changes - revert changes - kubectl rollout undo deployment/myapp-deployment - Undo commands to previous revisions will set the previous revision as the most recent in kubectl history if changes are tracked with --record","title":"Deployments"},{"location":"k8/#networking","text":"- Nodes are given an ip address. Used to access and ssh into node - Node ip address can be 192.168.1.xxx for example. - Minikube runs its own node within the host system for example - Pods within nodes are given their own ip address as well - A 10.244.x.x ip is addressed to it and other pods in the node - As such pods can communicate to other pods with 10.244.0.0 - Although this is generally considered bad practice as pod ips can change and there are better ways to network. DON'T DO THIS - Multi-Node networking - k8 doesn't setup cluster networking for us. - k8 expects us to enable communication between pods without a NAT and nodes ability connect to all containers without NAT - Multiple pre-built tooling to do this for us - flannel, vmware nsxt, cilium, cicso aci networks, Calico - ex: For local systems: calico, flannel work - ex: for vmware: vmware nsxt","title":"Networking"},{"location":"k8/#services","text":"- Enable communication between components within and outside application - Help connect applications together with other applications and users - For example: Makes frontend-available to users and to backend containers. - Allow backend service to communicate to db ### External Communication - how do we access a wepage externally from running cluster and pod - EX: k8 cluster running locally - WITH A cluster running on 192.168.1.2, external host computer running on 192.168.1.10, pod running on 10.244.0.2 - Could ssh into k8 node at 192.168.1.2 we could curl the internal pod but this doesn't give us easy/true external access - We need something in the middle to map requests to the node from our external host to the pod - This is what k8 services do! - k8 service can listen to node port and forward it to the pod - called a NodePort service ### Service types - NodePort: maps requests between node and pods - ClusterIP: uses a virtual ip in the cluster to enable communication between different services running - LoadBalancer: Provisions a load balancer in supported cloud providers - distribute load accross servers ### NodePort - Map port on node to port on the pod - Involves three ports - pod port == TargetPort - service port (service is like a virtual server inside the node that has its own ip (cluster ip of the service)) == Port - node port = NodePort - with a range of ports from 30000 - 32767 ### Creating a service #### NodePort Service EX: service-definition.yml apiVersion: v1 kind: Service metadata: name: myapp-service spec: type: NodePort ports: - targetPort: 80 port: 80 nodePort: 30008 selector: app: myapp type: front-end - the only mandatory field is port - if no port is supplied for targetPort, then it will be assigned to port - if no nodePort is assigned, then a random one will be given - specify the pod with labels and selectors - create service with ```kubectl create -f service-definition.yml``` - see service with ```kubectl get services``` - We can now access the pod using the node ip address and targetPort - ex: ```curl http://192.168.1.2:30008``` - This example demonstrates a service mapped to single pod but this isn't always the case. I.E multiple pods - Example: multiple pods up for load-balance and redundency with the same selector and label - K8 would forward all pods to the service and node using a random session affinty load balance method - What about a deployment spread out between multiple nodes - K8 will create service that spans all nodes and maps targetport to same nodeport on all clusters #### Service Demo - ```kubectl get svc``` = ```kubectl get service``` - If on minkube, we can run the minikube service command to get the url of the service - select a service (such as myapp-service) from kubectl get svc and run: - ```minkube service myapp-service --url``` #### ClusterIP Service - used to setup communication between different running services using a virtual cluster ip - defualt type of service. If not specified in yaml will default to this - EX: apiVersion: v1 kind: Service metadata: name: back-end spec: type: ClusterIP ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end #### Services - Load Balancer - Common steps for networking in a cluster 1. Create deployments 2. Create services (clusterIP) 3. Create services (LoadBalancer) - Used to create custom url and \"domain to access application\" - Essentially acting as a load-balancer or proxy for the node urls (192.168.xxx.xxx) - LoadBalancers are available natively with cloud providers/will only work with supported cloud providers - in unsupported envs it would have the same affect as nodePort EX: apiVersion: v1 kind: Service metadata: name: myapp-service spec: type: LoadBalancer ports: - targetPort: 80 port: 80 nodePort: 30008 Create a service from another service using ```kubectl expose``` - example: kubectl expose deployment my-app-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > svc.yaml ### Microservices - docker run --links is used to link docker containers together - example to link web app to redis container already running - ```docker run -d --name=vote -p 5000:80 --link redis:redis voting app``` - maps redis container to a hostname, in this case redis - --link container:hostname - using the links option is deprecated!!! - Example goal - deploy containers, enable connectivity and external access - Steps - Deploy PODs - Connect services (ClusterIp) - Configure services for external access (NodePort) -","title":"Services"},{"location":"k8/#todo","text":"- research purpose of selector/matchlabel in replicaSets - what is replicasets.apps used for in kubectl cli/command line - All of this guide assumes you are in a k8 cluster. What about switching/managing the cluster - keep in mind name or labels (such as myapp) (need to clarify whether its the name of label) are used to specify objects in kubectl cli","title":"TODO"},{"location":"k8/#containers-docker","text":"","title":"Containers - Docker"},{"location":"k8/#container-orchestration","text":"","title":"Container Orchestration"},{"location":"k8/#demo-setup-kubernetes","text":"","title":"Demo - Setup kubernetes"},{"location":"k8/#kubernetes-concepts-pods-replicasets-deployment-services","text":"","title":"kubernetes concepts - PODs | ReplicaSets | Deployment | Services"},{"location":"k8/#networking-in-kubernetes","text":"","title":"Networking in kubernetes"},{"location":"k8/#kubernetes-management-kubectl","text":"","title":"kubernetes management - Kubectl"},{"location":"k8/#kubernetes-defintion-files-yaml","text":"","title":"kubernetes defintion files - YAML"},{"location":"k8/#kubernetes-on-cloud-awsgcp","text":"","title":"kubernetes on cloud - AWS/GCP"}]}